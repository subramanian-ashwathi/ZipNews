# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AVvKTfGYILqPTzq733qSHq21Um8DwZ59
"""

#pip install pyspark

import pyspark
import pandas as pd
import tensorflow as tf
from google.colab import files


from pyspark.sql import SparkSession

from pyspark import SparkContext

import warnings
warnings.filterwarnings('ignore')

spark_context_sc = SparkContext(master='local[3]')
spark_session = SparkSession.builder.appName("Categorizer").getOrCreate()

from google.colab import files
upload = files.upload()

df = spark_session.read.json("CategorizedNews.json")
df.head()

df.count()

df = df.limit(10000)
df.count()

df.groupby('category').count().sort('count', ascending=False).show()

from pyspark.sql import functions as fun
df = df.withColumn('description', fun.concat(fun.col('headline'),fun.lit(' '), fun.col('short_description')))

df = df.drop("headline", "link", "authors", "short_description", "date")
df.show()

from pyspark.ml.feature import IDF
from pyspark.ml.feature import CountVectorizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import Tokenizer
from pyspark.ml.feature import StopWordsRemover


df.columns

import pyspark.sql.functions as check

df = df.where(check.col('description').isNotNull())
df.show()

token_create = Tokenizer(inputCol='description',outputCol='tokens')
stopwordRemove = StopWordsRemover(inputCol='tokens',outputCol='removed_stopwords')
vector_create = CountVectorizer(inputCol='removed_stopwords',outputCol='extracted_features')
idf = IDF(inputCol='extracted_features',outputCol='vectorizedFeatures')

label_encoding = StringIndexer(inputCol='category',outputCol='category_number').fit(df)
label_encoding.transform(df).show(10)

# Assume label_encoding is your StringIndexer object
transformed_df = label_encoding.transform(df)

# Get unique categories
unique_categories = transformed_df.select('category_number', 'category').distinct().collect()
#name = transformed_df.select('category').distinct().collect()

# Print the unique categories
for row in unique_categories:
    print("Category Number", row.category_number, " Category Name", row.category)

label_encoding.labels

label_dict = {
    0.0: 'POLITICS',
    1.0: 'ENTERTAINMENT',
    2.0: 'WORLD NEWS',
    3.0: 'QUEER VOICES',
    4.0: 'COMEDY',
    5.0: 'BLACK VOICES',
    6.0: 'SPORTS',
    13.0: 'MEDIA',
    8.0: 'WOMEN',
    10.0: 'CRIME',
    11.0: 'BUSINESS',
    12.0: 'LATINO VOICES',
    13.0: 'IMPACT',
    14.0: 'RELIGION',
    15.0: 'TRAVEL',
    16.0: 'STYLE',
    18.0: 'PARENTS',
    19.0: 'TECH',
    20.0: 'HEALTHY LIVING',
    22.0: 'EDUCATION',
    23.0: 'TASTE',
    25.0: 'COLLEGE',
    17.0: 'GREEN',
    9.0: 'WEIRD NEWS',
    24.0: 'ARTS $ CULTURE',
    21.0: 'SCIENCE',
    7.0: 'MEDIA'
}

df = label_encoding.transform(df)
df.show(100)

from pyspark.sql.functions import rand

df = df.orderBy(rand(seed=21))
train_df = df.limit(int(df.count() * 0.70))
test_df = df.exceptAll(train_df)

#from pyspark.ml.classification import DecisionTreeClassifier
#dt = DecisionTreeClassifier(featuresCol='vectorizedFeatures',labelCol='category_number')

from pyspark.ml.classification import LogisticRegression
logistic_reg = LogisticRegression(featuresCol='vectorizedFeatures',labelCol='category_number')

from pyspark.ml import Pipeline
cat_pipeline = Pipeline(stages = [token_create, stopwordRemove, vector_create,idf,logistic_reg])

cat_pipeline.stages

logistic_model = cat_pipeline.fit(train_df)

logistic_model
preds = logistic_model.transform(test_df)

preds.columns

preds.select('description','rawPrediction', 'probability','category','category_number','prediction').show(50)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='category_number')

evaluator.evaluate(preds)*100

from pyspark.mllib.evaluation import MulticlassMetrics
logistic_reg_metric = MulticlassMetrics(preds['category_number','prediction'].rdd)

print("Accuracy ", logistic_reg_metric.accuracy)
print("precision ", logistic_reg_metric.precision(1.0))
print("f1Score ", logistic_reg_metric.fMeasure(1.0))
print("recall ", logistic_reg_metric.recall(1.0))

"""We have built the model. Now using te model, we categorize the news from our dataset
**bold text**
"""

df2 = spark_session.read.csv("new_dataset.csv")
df2.show(5)

from pyspark.sql.functions import *

# Read CSV file with custom column names
df2 = spark_session.read.csv("new_dataset.csv", header=True, inferSchema=True).toDF(*["_c0", "_c1", "_c2","_c3","_c4","_c5","_c6"])

# Rename columns to match Pandas DataFrame column names
df2 = df2.withColumnRenamed("_c0", "url")\
       .withColumnRenamed("_c1", "title")\
       .withColumnRenamed("_c2", "DomainCountryCode")\
       .withColumnRenamed("_c3", "location")\
       .withColumnRenamed("_c4", "contextual_text")\
       .withColumnRenamed("_c5", "state_code")\
       .withColumnRenamed("_c6", "category")

df2.show()

#df2 = df2.limit(1000)
#df2.count()

from pyspark.sql import functions as sf2
df2 = df2.withColumn('description', sf2.concat(sf2.col('title'),sf2.lit(' '), sf2.col('contextual_text')))

df2 = df2.select("description","category")
df2.show()

prediction_data = logistic_model.transform(df2)

prediction_data.columns

prediction_data.select('description','rawPrediction', 'probability','category','prediction').show(100)

#predictions_data.write.format("csv").option("header", "true").mode("overwrite").save("predictions.csv")
prediction_data.write.format('json').mode("overwrite").save('path/to/output/file.json')

import pandas as pd

predictions_data_pd = prediction_data.select("prediction").toPandas()

predictions_data_pd.to_csv("path/to/output/prediction_cat.csv", index=False)

from google.colab import files
files.download('path/to/output/prediction_cat.csv')